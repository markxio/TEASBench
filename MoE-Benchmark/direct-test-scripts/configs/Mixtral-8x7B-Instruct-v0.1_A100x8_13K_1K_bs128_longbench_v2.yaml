
apiVersion: batch/v1
kind: Job
metadata:
  generateName: sglang-moe-cap-
  #generateName: sglang-moe-cap-mixtral-8x7b-instruct-v0-1-nvidia-a100-sxm4-80gbx8-13000-1000-bs128-longbench-v2-
  labels:
    kueue.x-k8s.io/queue-name:  eidf230ns-user-queue
spec:
  completions: 1
  backoffLimit: 0
  ttlSecondsAfterFinished: 1800
  template:
    metadata:
      name: job-sglang-moe-cap
      #name: job-sglang-moe-cap-mixtral-8x7b-instruct-v0-1-nvidia-a100-sxm4-80gbx8-13000-1000-bs128-longbench-v2
    spec:
      containers:
      - name: sglang-server
        image: lmsysorg/sglang:latest
        imagePullPolicy: IfNotPresent
        env:
          - name: SGLANG_EXPERT_DISTRIBUTION_RECORDER_DIR
            value: "/dev/shm/sglang_expert_distribution_recorder"
        command: ["/bin/bash", "-c"]
        args:
          - |
            apt-get update
            apt-get -y install git
            git clone https://github.com/markxio/MoE-CAP.git /dev/shm/MoE-CAP
            cd /dev/shm/MoE-CAP
            pip install -e .
            pip install gputil

            # Start server
            python -m moe_cap.systems.sglang \
              --model-path mistralai/Mixtral-8x7B-Instruct-v0.1 \
              --port 30000 \
              --expert-distribution-recorder-mode stat \
              --tp-size 8 \
              &> /dev/shm/Mixtral-8x7B-Instruct-v0-1_NVIDIA-A100-SXM4-80GBx8_13000_1000_bs128_longbench_v2_20260206-0915.server_log &
            SERVER_PID=$!

            # Wait until the /health endpoint returns HTTP 200
            echo "Waiting for SGLang server to be ready..."

            until curl -s -f http://localhost:30000/health > /dev/null; do
              echo -n "."
              sleep 2
            done

            echo "SGLang server is ready!"
            echo "Starting to serve bench (sending http requests)..."
            
            mkdir -p /dev/shm/Mixtral-8x7B-Instruct-v0-1_NVIDIA-A100-SXM4-80GBx8_13000_1000_bs128_longbench_v2
            python -m moe_cap.runner.openai_api_profile \
              --model_name mistralai/Mixtral-8x7B-Instruct-v0.1 \
              --datasets longbench_v2 \
              --input-tokens 13000 \
              --output-tokens 1000 \
              --num-samples 256 \
              --config-file configs/stub.yaml \
              --api-url http://localhost:30000/v1/completions \
              --backend sglang \
              --ignore-eos \
              --server-batch-size 128 \
              --output_dir /dev/shm/Mixtral-8x7B-Instruct-v0-1_NVIDIA-A100-SXM4-80GBx8_13000_1000_bs128_longbench_v2 \
              &> /dev/shm/Mixtral-8x7B-Instruct-v0-1_NVIDIA-A100-SXM4-80GBx8_13000_1000_bs128_longbench_v2_20260206-0915.client_log

            echo "Starting to serve bench (sending http requests)... done!"
            echo "Benchmark finished, shutting down server..."

            kill $SERVER_PID
            wait $SERVER_PID
            
            echo "Server stopped. Copying files to pvc..."
            
            mkdir -p /mnt/ceph/tmp/MoE-CAP-outputs
            cp -R /dev/shm/Mixtral-8x7B-Instruct-v0-1_NVIDIA-A100-SXM4-80GBx8_13000_1000_bs128_longbench_v2 /mnt/ceph/tmp/MoE-CAP-outputs/num_samples_256/
            cp /dev/shm/Mixtral-8x7B-Instruct-v0-1_NVIDIA-A100-SXM4-80GBx8_13000_1000_bs128_longbench_v2_20260206-0915* /mnt/ceph/tmp/MoE-CAP-outputs/num_samples_256/
            
            echo "Files copied, exiting container"
        ports:
          - containerPort: 30000 
        resources:
          requests:
            cpu: 10
            memory: '100Gi'
          limits:
            cpu: 10
            memory: '100Gi'
            nvidia.com/gpu: 8
        volumeMounts:
          - mountPath: /mnt/ceph
            name: volume
          - mountPath: /dev/shm
            name: dshm
      restartPolicy: Never
      volumes:
        - name: volume
          persistentVolumeClaim:
            claimName: client-ceph-pvc
        - name: dshm
          emptyDir:
            medium: Memory
            sizeLimit: 16Gi
      nodeSelector:
        nvidia.com/gpu.product: NVIDIA-A100-SXM4-80GB
               